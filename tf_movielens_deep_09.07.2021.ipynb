{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab6d64b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a440ec25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'> <class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7]),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n",
      "{'bucketized_user_age': 25.0,\n",
      " 'movie_genres': array([ 4, 14]),\n",
      " 'movie_id': b'709',\n",
      " 'movie_title': b'Strictly Ballroom (1992)',\n",
      " 'raw_user_age': 32.0,\n",
      " 'timestamp': 875654590,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'92',\n",
      " 'user_occupation_label': 5,\n",
      " 'user_occupation_text': b'entertainment',\n",
      " 'user_rating': 2.0,\n",
      " 'user_zip_code': b'80525'}\n",
      "{'movie_genres': array([4]),\n",
      " 'movie_id': b'1681',\n",
      " 'movie_title': b'You So Crazy (1994)'}\n",
      "{'movie_genres': array([4, 7]),\n",
      " 'movie_id': b'1457',\n",
      " 'movie_title': b'Love Is All There Is (1996)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-08 18:12:23.641709: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-09-08 18:12:23.676380: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
    "print(type(ratings), type(movies))\n",
    "for i in ratings.take(2).as_numpy_iterator():\n",
    "    pprint.pprint(i)\n",
    "for i in movies.take(2).as_numpy_iterator():\n",
    "    pprint.pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc779f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[8, 9, 3, 4], [], [1, 0], [-6], []]>\n"
     ]
    }
   ],
   "source": [
    "numbers = tf.ragged.constant([[8, 9, 3, 4], [], [1, 0], [-6], []])\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3cc0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
    "\n",
    "#maxlen = 5\n",
    "#padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(ratings.map(lambda x: x[\"movie_genres\"]), maxlen=maxlen, padding=\"post\")\n",
    "#padded_inputs = tf.data.Dataset.from_tensor_slices({\"padded_genres\": padded_inputs})\n",
    "\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"timestamp\": x[\"timestamp\"],\n",
    "    \"movie_id\": x[\"movie_id\"],\n",
    "    \"movie_genres\": x[\"movie_genres\"]\n",
    "})\n",
    "\n",
    "movies = movies.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"movie_id\": x[\"movie_id\"],\n",
    "    \"movie_genres\": x[\"movie_genres\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6917b9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_genres': array([7]),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'timestamp': 879024327,\n",
      " 'user_id': b'138'}\n",
      "{'movie_genres': array([ 4, 14]),\n",
      " 'movie_id': b'709',\n",
      " 'movie_title': b'Strictly Ballroom (1992)',\n",
      " 'timestamp': 875654590,\n",
      " 'user_id': b'92'}\n",
      "{'movie_genres': array([4]),\n",
      " 'movie_id': b'1681',\n",
      " 'movie_title': b'You So Crazy (1994)'}\n",
      "{'movie_genres': array([4, 7]),\n",
      " 'movie_id': b'1457',\n",
      " 'movie_title': b'Love Is All There Is (1996)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-08 17:51:14.722316: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-09-08 17:51:14.773329: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for i in ratings.take(2).as_numpy_iterator():\n",
    "    pprint.pprint(i)\n",
    "for i in movies.take(2).as_numpy_iterator():\n",
    "    pprint.pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e6f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = movies.map(lambda x: x[\"movie_genres\"])\n",
    "for i in a.take(5).as_numpy_iterator():\n",
    "    print(i, i.shape, i[0], type(i[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d35f37",
   "metadata": {},
   "source": [
    "### Reconstruct the tf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d7c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_list = list(ratings.as_numpy_iterator())\n",
    "movies_list = list(movies.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "50493a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 [{'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\", 'user_id': b'138', 'timestamp': 879024327, 'movie_id': b'357', 'movie_genres': array([7])}, {'movie_title': b'Strictly Ballroom (1992)', 'user_id': b'92', 'timestamp': 875654590, 'movie_id': b'709', 'movie_genres': array([ 4, 14])}]\n",
      "1682 [{'movie_title': b'You So Crazy (1994)', 'movie_id': b'1681', 'movie_genres': array([4])}, {'movie_title': b'Love Is All There Is (1996)', 'movie_id': b'1457', 'movie_genres': array([4, 7])}]\n"
     ]
    }
   ],
   "source": [
    "print(len(ratings_list), ratings_list[:2])\n",
    "print(len(movies_list), movies_list[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b92da7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_keys = ['movie_title', 'user_id', 'timestamp', 'movie_id', 'movie_genres']\n",
    "movie_keys = ['movie_title', 'movie_id', 'movie_genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "36a7ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ratings = {j: [i[j] for i in ratings_list] for j in rating_keys}\n",
    "new_movies = {j: [i[j] for i in movies_list] for j in movie_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "76c69c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ratings['movie_genres'] = [','.join([str(j) for j in list(i)]) for i in new_ratings['movie_genres']]\n",
    "new_movies['movie_genres'] = [','.join([str(j) for j in list(i)]) for i in new_movies['movie_genres']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "56d44523",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = tf.data.Dataset.from_tensor_slices({'movie_title': new_ratings['movie_title'], \n",
    "                                              'user_id': new_ratings['user_id'], \n",
    "                                              'timestamp': new_ratings['timestamp'], \n",
    "                                              'movie_id': new_ratings['movie_id'],\n",
    "                                             'movie_genres': new_ratings['movie_genres']})\n",
    "movies = tf.data.Dataset.from_tensor_slices(new_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "13c326bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = np.concatenate(list(ratings.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
    "\n",
    "max_timestamp = timestamps.max()\n",
    "min_timestamp = timestamps.min()\n",
    "\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000,\n",
    ")\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movies.map(lambda x: x['movie_title']).batch(1000))))\n",
    "unique_movie_ids = np.unique(np.concatenate(list(movies.map(lambda x: x['movie_id']).batch(1000))))\n",
    "# unique_movie_genres = np.unique(np.concatenate(list(movies.map(lambda x: x[\"movie_genres\"]))))\n",
    "unique_user_ids = np.unique(np.concatenate(list(ratings.map(lambda x: x[\"user_id\"]).batch(1000))))\n",
    "\n",
    "#unique_movie_genres = np.unique(np.concatenate(list(movies.map(lambda x: x[\"movie_genres\"]))))\n",
    "#unique_movie_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c2a10f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'4'\n",
      "b'4,7'\n",
      "b'1,3'\n",
      "b'0'\n",
      "b'7'\n"
     ]
    }
   ],
   "source": [
    "a = movies.map(lambda x: x[\"movie_genres\"])\n",
    "for i in a.take(5).as_numpy_iterator():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec262a",
   "metadata": {},
   "source": [
    "### Query model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dea954fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.user_embedding = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "            vocabulary=unique_user_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "    ])\n",
    "    self.timestamp_embedding = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
    "        tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
    "    ])\n",
    "    self.normalized_timestamp = tf.keras.layers.experimental.preprocessing.Normalization(axis=None)\n",
    "    self.normalized_timestamp.adapt(timestamps)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Take the input dictionary, pass it through each input layer,\n",
    "    # and concatenate the result.\n",
    "    return tf.concat([\n",
    "        self.user_embedding(inputs[\"user_id\"]),\n",
    "        self.timestamp_embedding(inputs[\"timestamp\"]),\n",
    "        #self.normalized_timestamp(inputs[\"timestamp\"]),\n",
    "        tf.reshape(self.normalized_timestamp(inputs[\"timestamp\"]), (-1, 1)),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3c22ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryModel(tf.keras.Model):\n",
    "  \"\"\"Model for encoding user queries.\"\"\"\n",
    "\n",
    "  def __init__(self, layer_sizes):\n",
    "    \"\"\"Model for encoding user queries.\n",
    "    Args:\n",
    "      layer_sizes:\n",
    "        A list of integers where the i-th entry represents the number of units\n",
    "        the i-th layer contains.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    # We first use the user model for generating embeddings.\n",
    "    self.embedding_model = UserModel()\n",
    "\n",
    "    # Then construct the layers.\n",
    "    self.dense_layers = tf.keras.Sequential()\n",
    "\n",
    "    # Use the ReLU activation for all but the last layer.\n",
    "    for layer_size in layer_sizes[:-1]:\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "\n",
    "    # No activation for the last layer.\n",
    "    for layer_size in layer_sizes[-1:]:\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "\n",
    "  def call(self, inputs):\n",
    "    \"\"\"\n",
    "    Inputs embedding and go through the dense layers\n",
    "    Arguments:\n",
    "      -- inputs: tf dictionary - tensor dictionary contains multiple features of users\n",
    "    Return:\n",
    "      -- dense_layers(feature_embedding): tf tensor - values of dense layers\n",
    "    \"\"\"\n",
    "    feature_embedding = self.embedding_model(inputs)\n",
    "    return self.dense_layers(feature_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa746a",
   "metadata": {},
   "source": [
    "### Candidate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa6bced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_text = tf.keras.layers.experimental.preprocessing.TextVectorization()\n",
    "title_text.adapt(ratings.map(lambda x: x[\"movie_title\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65df7ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b\"One Flew Over the Cuckoo's Nest (1975)\"], shape=(1,), dtype=string) tf.Tensor([[1 1 1 2 1 1 1]], shape=(1, 7), dtype=int64)\n",
      "tf.Tensor([b'Strictly Ballroom (1992)'], shape=(1,), dtype=string) tf.Tensor([[1 1 1]], shape=(1, 3), dtype=int64)\n",
      "tf.Tensor([b'Very Brady Sequel, A (1996)'], shape=(1,), dtype=string) tf.Tensor([[1 1 1 1 1]], shape=(1, 5), dtype=int64)\n",
      "tf.Tensor([b'Pulp Fiction (1994)'], shape=(1,), dtype=string) tf.Tensor([[1 1 1]], shape=(1, 3), dtype=int64)\n",
      "tf.Tensor([b'Scream 2 (1997)'], shape=(1,), dtype=string) tf.Tensor([[1 1 1]], shape=(1, 3), dtype=int64)\n",
      "tf.Tensor([b'Crash (1996)'], shape=(1,), dtype=string) tf.Tensor([[1 1]], shape=(1, 2), dtype=int64)\n",
      "tf.Tensor([b'Aladdin (1992)'], shape=(1,), dtype=string) tf.Tensor([[1 1]], shape=(1, 2), dtype=int64)\n",
      "tf.Tensor([b'True Romance (1993)'], shape=(1,), dtype=string) tf.Tensor([[1 1 1]], shape=(1, 3), dtype=int64)\n",
      "tf.Tensor([b'Bob Roberts (1992)'], shape=(1,), dtype=string) tf.Tensor([[1 1 1]], shape=(1, 3), dtype=int64)\n",
      "tf.Tensor([b'Starship Troopers (1997)'], shape=(1,), dtype=string) tf.Tensor([[1 1 1]], shape=(1, 3), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for row in ratings.batch(1).map(lambda x: x[\"movie_title\"]).take(10):\n",
    "  print(row, title_text(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "53d3be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    max_tokens = 10_000\n",
    "    \n",
    "    # movie_\n",
    "    self.title_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "          vocabulary=unique_movie_titles, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, 32)\n",
    "    ]) # embedding has no input_length\n",
    "\n",
    "    \n",
    "    self.title_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=max_tokens)\n",
    "    self.title_text_embedding = tf.keras.Sequential([\n",
    "      self.title_vectorizer,\n",
    "      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "      tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    ])\n",
    "    self.title_vectorizer.adapt(movies.map(lambda x: x['movie_title']))\n",
    "    \n",
    "\n",
    "    self.id_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "          vocabulary=unique_movie_ids, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_movie_ids) + 1, 32)\n",
    "    ])\n",
    "    \n",
    "    \"\"\" array of genres \"\"\" \n",
    "    #genre_num = 5\n",
    "    #self.genres_id_lookup = tf.keras.layers.experimental.preprocessing.IntegerLookup()\n",
    "    #self.genres_embedding = tf.keras.Sequential([\n",
    "      #self.genres_id_lookup,\n",
    "      #tf.keras.layers.Embedding(len(unique_movie_genres)+1, 32, mask_zero=True),\n",
    "      #tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    #])\n",
    "    #self.genres_id_lookup.adapt(movies.map(lambda x: x[\"movie_genres\"]))\n",
    "    \n",
    "    max_genres = 22\n",
    "    self.genres_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=max_genres)\n",
    "    self.genres_embedding = tf.keras.Sequential([\n",
    "      self.genres_vectorizer,\n",
    "      tf.keras.layers.Embedding(max_genres, 32, mask_zero=True),\n",
    "      tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    ])\n",
    "    self.genres_vectorizer.adapt(movies.map(lambda x: x['movie_title']))\n",
    "    \n",
    "\n",
    "  def call(self, titles):\n",
    "    \"\"\"\n",
    "    32-dimension embeddings of each feature are concatenated (32 X number of features)\n",
    "    return:\n",
    "      -- Concatenated (32 X number of features)\n",
    "    \"\"\"\n",
    "    return tf.concat([\n",
    "        self.title_embedding(titles['movie_title']),\n",
    "        self.title_text_embedding(titles['movie_title']),\n",
    "        self.id_embedding(titles['movie_id']),\n",
    "        self.genres_embedding(titles['movie_genres']),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0e36b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateModel(tf.keras.Model):\n",
    "  \"\"\"Model for encoding movies.\"\"\"\n",
    "\n",
    "  def __init__(self, layer_sizes):\n",
    "    \"\"\"Model for encoding movies.\n",
    "\n",
    "    Args:\n",
    "      layer_sizes:\n",
    "        A list of integers where the i-th entry represents the number of units\n",
    "        the i-th layer contains.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding_model = MovieModel()\n",
    "\n",
    "    # Then construct the layers.\n",
    "    self.dense_layers = tf.keras.Sequential()\n",
    "\n",
    "    # Use the ReLU activation for all but the last layer.\n",
    "    for layer_size in layer_sizes[:-1]:\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "\n",
    "    # No activation for the last layer.\n",
    "    for layer_size in layer_sizes[-1:]:\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "\n",
    "  def call(self, inputs):\n",
    "    \"\"\"\n",
    "    Inputs embedding and go through the dense layers\n",
    "    Arguments:\n",
    "      -- inputs: tf dictionary - tensor dictionary contains multiple features of movies\n",
    "    Return:\n",
    "      -- dense_layers(feature_embedding): tf tensor - values of dense layers\n",
    "    \"\"\"\n",
    "    feature_embedding = self.embedding_model(inputs)\n",
    "    return self.dense_layers(feature_embedding)\n",
    "    # return feature_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ee59d",
   "metadata": {},
   "source": [
    "### Combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c6493a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [36]\n",
    "candidate_model = CandidateModel(layer_sizes)\n",
    "candidates = movies.batch(128).map(candidate_model)\n",
    "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
    "            candidates=movies.batch(128).map(candidate_model),),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "beda55fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 128 (128,)\n",
      "[[-0.01146629  0.0494458  -0.0304136  ...  0.00677372  0.00883717\n",
      "   0.0170493 ]\n",
      " [-0.04056208 -0.014879    0.01497013 ...  0.00677372  0.00883717\n",
      "   0.0170493 ]\n",
      " [-0.0006185   0.02809947 -0.00539805 ...  0.00677372  0.00883717\n",
      "   0.0170493 ]\n",
      " ...\n",
      " [-0.00436002 -0.0043005  -0.01528854 ...  0.00677372  0.00883717\n",
      "   0.0170493 ]\n",
      " [-0.03700734  0.01654127 -0.04621185 ...  0.00677372  0.00883717\n",
      "   0.0170493 ]\n",
      " [-0.01145233  0.00814867  0.04739863 ...  0.00677372  0.00883717\n",
      "   0.0170493 ]]\n",
      "128 128 (128,)\n",
      "[[-0.02472454 -0.00952109  0.0455688  ...  0.00677372  0.00883717\n",
      "   0.0170493 ]\n",
      " [ 0.03062434 -0.04533291  0.03164179 ...  0.00677372  0.00883717\n",
      "   0.0170493 ]\n",
      " [ 0.01599726 -0.03422933 -0.04874967 ...  0.00677372  0.00883717\n",
      "   0.0170493 ]\n",
      " ...\n",
      " [-0.01026998 -0.00755505 -0.04873674 ...  0.00677372  0.00883717\n",
      "   0.0170493 ]\n",
      " [-0.00827784  0.01520301 -0.00301795 ...  0.00677372  0.00883717\n",
      "   0.0170493 ]\n",
      " [-0.03011192 -0.04462643 -0.0056051  ...  0.00677372  0.00883717\n",
      "   0.0170493 ]]\n"
     ]
    }
   ],
   "source": [
    "for i in candidates.take(2).as_numpy_iterator():\n",
    "    print(len(i), len(i[0]), i[0].shape)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2fc7cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self, layer_sizes):\n",
    "    super().__init__()\n",
    "    self.query_model = QueryModel(layer_sizes)\n",
    "    self.candidate_model = CandidateModel(layer_sizes)\n",
    "    self.task = tfrs.tasks.Retrieval(\n",
    "        metrics=tfrs.metrics.FactorizedTopK(\n",
    "            candidates=movies.batch(128).map(self.candidate_model),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    # We only pass the user id and timestamp features into the query model. This\n",
    "    # is to ensure that the training inputs would have the same keys as the\n",
    "    # query inputs. Otherwise the discrepancy in input structure would cause an\n",
    "    # error when loading the query model after saving it.\n",
    "    query_embeddings = self.query_model({\n",
    "        \"user_id\": features[\"user_id\"],\n",
    "        \"timestamp\": features[\"timestamp\"],\n",
    "    })\n",
    "    movie_embeddings = self.candidate_model({\n",
    "        \"movie_id\": features[\"movie_id\"],\n",
    "        \"movie_title\": features[\"movie_title\"],\n",
    "        \"movie_genres\": features[\"movie_genres\"],\n",
    "    })     # \"\"\" This is where the problem is \"\"\"\n",
    "\n",
    "    return self.task(query_embeddings, movie_embeddings, compute_metrics=not training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3bba4dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(2048)\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "58a6ffdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_genres': b'7',\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'timestamp': 879024327,\n",
      " 'user_id': b'138'}\n",
      "{'movie_genres': b'4,14',\n",
      " 'movie_id': b'709',\n",
      " 'movie_title': b'Strictly Ballroom (1992)',\n",
      " 'timestamp': 875654590,\n",
      " 'user_id': b'92'}\n"
     ]
    }
   ],
   "source": [
    "for i in ratings.take(2).as_numpy_iterator():\n",
    "    pprint.pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ba9ffe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_genres': array([b'0,5,7', b'1,3,8', b'5,7,16', ..., b'7', b'4', b'10,16'],\n",
      "      dtype=object),\n",
      " 'movie_id': array([b'127', b'1133', b'100', ..., b'366', b'367', b'288'], dtype=object),\n",
      " 'movie_title': array([b'Godfather, The (1972)', b'Escape to Witch Mountain (1975)',\n",
      "       b'Fargo (1996)', ..., b'Dangerous Minds (1995)',\n",
      "       b'Clueless (1995)', b'Scream (1996)'], dtype=object),\n",
      " 'timestamp': array([880859493, 882386848, 879442537, ..., 891310875, 892836551,\n",
      "       889388862], dtype=int32),\n",
      " 'user_id': array([b'424', b'429', b'53', ..., b'766', b'125', b'478'], dtype=object)}\n",
      "{'movie_genres': array([b'13,16', b'12,14', b'0,14,16', ..., b'5,7,14,16', b'5,7,9', b'4'],\n",
      "      dtype=object),\n",
      " 'movie_id': array([b'479', b'705', b'748', ..., b'129', b'299', b'368'], dtype=object),\n",
      " 'movie_title': array([b'Vertigo (1958)', b\"Singin' in the Rain (1952)\",\n",
      "       b'Saint, The (1997)', ..., b'Bound (1996)', b'Hoodlum (1997)',\n",
      "       b'Bio-Dome (1996)'], dtype=object),\n",
      " 'timestamp': array([879875432, 879434666, 875333873, ..., 889492503, 891447522,\n",
      "       886921977], dtype=int32),\n",
      " 'user_id': array([b'326', b'370', b'287', ..., b'663', b'489', b'595'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "for i in cached_train.take(2).as_numpy_iterator():\n",
    "    pprint.pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "78e0c6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "40/40 [==============================] - 3s 42ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 15162.7701 - regularization_loss: 0.0000e+00 - total_loss: 15162.7701\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 5s 114ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 14516.4145 - regularization_loss: 0.0000e+00 - total_loss: 14516.4145 - val_factorized_top_k/top_1_categorical_accuracy: 5.5000e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0031 - val_factorized_top_k/top_10_categorical_accuracy: 0.0074 - val_factorized_top_k/top_50_categorical_accuracy: 0.0489 - val_factorized_top_k/top_100_categorical_accuracy: 0.1161 - val_loss: 28711.5488 - val_regularization_loss: 0.0000e+00 - val_total_loss: 28711.5488\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 2s 41ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 14320.8016 - regularization_loss: 0.0000e+00 - total_loss: 14320.8016\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 3s 80ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 14186.2116 - regularization_loss: 0.0000e+00 - total_loss: 14186.2116 - val_factorized_top_k/top_1_categorical_accuracy: 3.0000e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0034 - val_factorized_top_k/top_10_categorical_accuracy: 0.0092 - val_factorized_top_k/top_50_categorical_accuracy: 0.0806 - val_factorized_top_k/top_100_categorical_accuracy: 0.1679 - val_loss: 28287.9844 - val_regularization_loss: 0.0000e+00 - val_total_loss: 28287.9844\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 2s 39ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 14044.8694 - regularization_loss: 0.0000e+00 - total_loss: 14044.8694\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 3s 79ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13943.0408 - regularization_loss: 0.0000e+00 - total_loss: 13943.0408 - val_factorized_top_k/top_1_categorical_accuracy: 5.0000e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0039 - val_factorized_top_k/top_10_categorical_accuracy: 0.0099 - val_factorized_top_k/top_50_categorical_accuracy: 0.0849 - val_factorized_top_k/top_100_categorical_accuracy: 0.1812 - val_loss: 28098.1230 - val_regularization_loss: 0.0000e+00 - val_total_loss: 28098.1230\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - 2s 39ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13876.4858 - regularization_loss: 0.0000e+00 - total_loss: 13876.4858\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - 3s 79ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13812.3035 - regularization_loss: 0.0000e+00 - total_loss: 13812.3035 - val_factorized_top_k/top_1_categorical_accuracy: 8.5000e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0068 - val_factorized_top_k/top_10_categorical_accuracy: 0.0165 - val_factorized_top_k/top_50_categorical_accuracy: 0.1009 - val_factorized_top_k/top_100_categorical_accuracy: 0.2027 - val_loss: 28003.4805 - val_regularization_loss: 0.0000e+00 - val_total_loss: 28003.4805\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - 2s 40ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13760.6572 - regularization_loss: 0.0000e+00 - total_loss: 13760.6572\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - 3s 80ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13713.1962 - regularization_loss: 0.0000e+00 - total_loss: 13713.1962 - val_factorized_top_k/top_1_categorical_accuracy: 0.0011 - val_factorized_top_k/top_5_categorical_accuracy: 0.0081 - val_factorized_top_k/top_10_categorical_accuracy: 0.0173 - val_factorized_top_k/top_50_categorical_accuracy: 0.1072 - val_factorized_top_k/top_100_categorical_accuracy: 0.2149 - val_loss: 27957.6855 - val_regularization_loss: 0.0000e+00 - val_total_loss: 27957.6855\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - 2s 40ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13676.0691 - regularization_loss: 0.0000e+00 - total_loss: 13676.0691\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - 4s 81ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13642.5236 - regularization_loss: 0.0000e+00 - total_loss: 13642.5236 - val_factorized_top_k/top_1_categorical_accuracy: 4.0000e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0074 - val_factorized_top_k/top_10_categorical_accuracy: 0.0174 - val_factorized_top_k/top_50_categorical_accuracy: 0.1127 - val_factorized_top_k/top_100_categorical_accuracy: 0.2258 - val_loss: 27936.3711 - val_regularization_loss: 0.0000e+00 - val_total_loss: 27936.3711\n",
      "Epoch 13/20\n",
      "40/40 [==============================] - 2s 39ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13609.0912 - regularization_loss: 0.0000e+00 - total_loss: 13609.0912\n",
      "Epoch 14/20\n",
      "40/40 [==============================] - 3s 79ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13583.3907 - regularization_loss: 0.0000e+00 - total_loss: 13583.3907 - val_factorized_top_k/top_1_categorical_accuracy: 6.0000e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0078 - val_factorized_top_k/top_10_categorical_accuracy: 0.0182 - val_factorized_top_k/top_50_categorical_accuracy: 0.1182 - val_factorized_top_k/top_100_categorical_accuracy: 0.2379 - val_loss: 27918.5781 - val_regularization_loss: 0.0000e+00 - val_total_loss: 27918.5781\n",
      "Epoch 15/20\n",
      "40/40 [==============================] - 2s 39ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13557.6400 - regularization_loss: 0.0000e+00 - total_loss: 13557.6400\n",
      "Epoch 16/20\n",
      "40/40 [==============================] - 3s 81ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13534.7481 - regularization_loss: 0.0000e+00 - total_loss: 13534.7481 - val_factorized_top_k/top_1_categorical_accuracy: 7.0000e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0080 - val_factorized_top_k/top_10_categorical_accuracy: 0.0188 - val_factorized_top_k/top_50_categorical_accuracy: 0.1231 - val_factorized_top_k/top_100_categorical_accuracy: 0.2437 - val_loss: 27926.8945 - val_regularization_loss: 0.0000e+00 - val_total_loss: 27926.8945\n",
      "Epoch 17/20\n",
      "40/40 [==============================] - 2s 40ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13512.8700 - regularization_loss: 0.0000e+00 - total_loss: 13512.8700\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - 3s 80ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13489.9106 - regularization_loss: 0.0000e+00 - total_loss: 13489.9106 - val_factorized_top_k/top_1_categorical_accuracy: 9.5000e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0071 - val_factorized_top_k/top_10_categorical_accuracy: 0.0174 - val_factorized_top_k/top_50_categorical_accuracy: 0.1247 - val_factorized_top_k/top_100_categorical_accuracy: 0.2456 - val_loss: 27933.7539 - val_regularization_loss: 0.0000e+00 - val_total_loss: 27933.7539\n",
      "Epoch 19/20\n",
      "40/40 [==============================] - 2s 38ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13472.1598 - regularization_loss: 0.0000e+00 - total_loss: 13472.1598\n",
      "Epoch 20/20\n",
      "40/40 [==============================] - 3s 80ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13456.7019 - regularization_loss: 0.0000e+00 - total_loss: 13456.7019 - val_factorized_top_k/top_1_categorical_accuracy: 0.0011 - val_factorized_top_k/top_5_categorical_accuracy: 0.0077 - val_factorized_top_k/top_10_categorical_accuracy: 0.0179 - val_factorized_top_k/top_50_categorical_accuracy: 0.1203 - val_factorized_top_k/top_100_categorical_accuracy: 0.2462 - val_loss: 27945.9219 - val_regularization_loss: 0.0000e+00 - val_total_loss: 27945.9219\n",
      "Top-100 accuracy: 0.25.\n",
      "5/5 [==============================] - 2s 312ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0011 - factorized_top_k/top_5_categorical_accuracy: 0.0077 - factorized_top_k/top_10_categorical_accuracy: 0.0179 - factorized_top_k/top_50_categorical_accuracy: 0.1203 - factorized_top_k/top_100_categorical_accuracy: 0.2462 - loss: 30746.9870 - regularization_loss: 0.0000e+00 - total_loss: 30746.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.0010999999940395355,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.007699999958276749,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.017899999395012856,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.12030000239610672,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.24619999527931213,\n",
       " 'loss': 27945.921875,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 27945.921875}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "model = MovielensModel([64,32])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "one_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=2,\n",
    "    epochs=num_epochs,\n",
    "    verbose=1)\n",
    "\n",
    "accuracy = one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Top-100 accuracy: {accuracy:.2f}.\")\n",
    "\n",
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa9ea6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m78"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
