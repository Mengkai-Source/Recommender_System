{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a9f71f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3de72265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7]),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n",
      "{'movie_genres': array([4]),\n",
      " 'movie_id': b'1681',\n",
      " 'movie_title': b'You So Crazy (1994)'}\n"
     ]
    }
   ],
   "source": [
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
    "for i in ratings.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(i)\n",
    "for i in movies.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6fd18302",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
    "\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"timestamp\": x[\"timestamp\"],\n",
    "    \"movie_id\": x[\"movie_id\"],\n",
    "})\n",
    "# movies_feature = movies\n",
    "# movies = movies.map(lambda x: x[\"movie_title\"])\n",
    "movies = movies.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"movie_id\": x[\"movie_id\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "412f33fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'timestamp': 879024327,\n",
      " 'user_id': b'138'}\n",
      "{'movie_id': b'709',\n",
      " 'movie_title': b'Strictly Ballroom (1992)',\n",
      " 'timestamp': 875654590,\n",
      " 'user_id': b'92'}\n",
      "{'movie_id': b'1681', 'movie_title': b'You So Crazy (1994)'}\n",
      "{'movie_id': b'1457', 'movie_title': b'Love Is All There Is (1996)'}\n"
     ]
    }
   ],
   "source": [
    "for i in ratings.take(2).as_numpy_iterator():\n",
    "    pprint.pprint(i)\n",
    "for i in movies.take(2).as_numpy_iterator():\n",
    "    pprint.pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8ca7f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = np.concatenate(list(ratings.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
    "\n",
    "max_timestamp = timestamps.max()\n",
    "min_timestamp = timestamps.min()\n",
    "\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000,\n",
    ")\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movies.map(lambda x: x['movie_title']).batch(1000))))\n",
    "unique_movie_ids = np.unique(np.concatenate(list(movies.map(lambda x: x['movie_id']).batch(1000))))\n",
    "unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n",
    "    lambda x: x[\"user_id\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9daded",
   "metadata": {},
   "source": [
    "### Query model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5b50fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.user_embedding = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "            vocabulary=unique_user_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "    ])\n",
    "    self.timestamp_embedding = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
    "        tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
    "    ])\n",
    "    self.normalized_timestamp = tf.keras.layers.experimental.preprocessing.Normalization(axis=None)\n",
    "    self.normalized_timestamp.adapt(timestamps)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Take the input dictionary, pass it through each input layer,\n",
    "    # and concatenate the result.\n",
    "    return tf.concat([\n",
    "        self.user_embedding(inputs[\"user_id\"]),\n",
    "        self.timestamp_embedding(inputs[\"timestamp\"]),\n",
    "        #self.normalized_timestamp(inputs[\"timestamp\"]),\n",
    "        tf.reshape(self.normalized_timestamp(inputs[\"timestamp\"]), (-1, 1)),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8840abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryModel(tf.keras.Model):\n",
    "  \"\"\"Model for encoding user queries.\"\"\"\n",
    "\n",
    "  def __init__(self, layer_sizes):\n",
    "    \"\"\"Model for encoding user queries.\n",
    "    Args:\n",
    "      layer_sizes:\n",
    "        A list of integers where the i-th entry represents the number of units\n",
    "        the i-th layer contains.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    # We first use the user model for generating embeddings.\n",
    "    self.embedding_model = UserModel()\n",
    "\n",
    "    # Then construct the layers.\n",
    "    self.dense_layers = tf.keras.Sequential()\n",
    "\n",
    "    # Use the ReLU activation for all but the last layer.\n",
    "    for layer_size in layer_sizes[:-1]:\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "\n",
    "    # No activation for the last layer.\n",
    "    for layer_size in layer_sizes[-1:]:\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "\n",
    "  def call(self, inputs):\n",
    "    \"\"\"\n",
    "    Inputs embedding and go through the dense layers\n",
    "    Arguments:\n",
    "      -- inputs: tf dictionary - tensor dictionary contains multiple features of users\n",
    "    Return:\n",
    "      -- dense_layers(feature_embedding): tf tensor - values of dense layers\n",
    "    \"\"\"\n",
    "    feature_embedding = self.embedding_model(inputs)\n",
    "    return self.dense_layers(feature_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e2a55",
   "metadata": {},
   "source": [
    "### Candidate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "20098af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    max_tokens = 10_000\n",
    "\n",
    "    self.title_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "          vocabulary=unique_movie_titles, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, 32)\n",
    "    ])\n",
    "\n",
    "    self.title_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "        max_tokens=max_tokens)\n",
    "\n",
    "    self.title_text_embedding = tf.keras.Sequential([\n",
    "      self.title_vectorizer,\n",
    "      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "      tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    ])\n",
    "\n",
    "    self.title_vectorizer.adapt(movies.map(lambda x: x['movie_title']))\n",
    "    \n",
    "    self.id_embedding = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "          vocabulary=unique_movie_ids, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_movie_ids) + 1, 32)\n",
    "    ])\n",
    "\n",
    "  def call(self, titles):\n",
    "    return tf.concat([\n",
    "        self.title_embedding(titles['movie_title']),\n",
    "        self.title_text_embedding(titles['movie_title']),\n",
    "        self.id_embedding(titles['movie_id']),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ed34b75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateModel(tf.keras.Model):\n",
    "  \"\"\"Model for encoding movies.\"\"\"\n",
    "\n",
    "  def __init__(self, layer_sizes):\n",
    "    \"\"\"Model for encoding movies.\n",
    "\n",
    "    Args:\n",
    "      layer_sizes:\n",
    "        A list of integers where the i-th entry represents the number of units\n",
    "        the i-th layer contains.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding_model = MovieModel()\n",
    "\n",
    "    # Then construct the layers.\n",
    "    self.dense_layers = tf.keras.Sequential()\n",
    "\n",
    "    # Use the ReLU activation for all but the last layer.\n",
    "    for layer_size in layer_sizes[:-1]:\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
    "\n",
    "    # No activation for the last layer.\n",
    "    for layer_size in layer_sizes[-1:]:\n",
    "        self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
    "\n",
    "  def call(self, inputs):\n",
    "    \"\"\"\n",
    "    Inputs embedding and go through the dense layers\n",
    "    Arguments:\n",
    "      -- inputs: tf dictionary - tensor dictionary contains multiple features of movies\n",
    "    Return:\n",
    "      -- dense_layers(feature_embedding): tf tensor - values of dense layers\n",
    "    \"\"\"\n",
    "    feature_embedding = self.embedding_model(inputs)\n",
    "    return self.dense_layers(feature_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de596b37",
   "metadata": {},
   "source": [
    "### Combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3aafcddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [32]\n",
    "candidate_model = CandidateModel(layer_sizes)\n",
    "candidates=movies.batch(128).map(candidate_model)\n",
    "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
    "            candidates=movies.batch(128).map(candidate_model),),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64e51759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 32\n",
      "[[-0.03409795 -0.03644416 -0.02098895 ... -0.01194747 -0.06696635\n",
      "   0.00989223]\n",
      " [-0.06053327 -0.01656315 -0.00986068 ... -0.02516921 -0.04961792\n",
      "  -0.02841071]\n",
      " [-0.00669995  0.01118299 -0.02884758 ...  0.00024492 -0.03789013\n",
      "  -0.02046539]\n",
      " ...\n",
      " [ 0.03002698 -0.03659357  0.01598025 ...  0.00870787 -0.00054528\n",
      "  -0.02680092]\n",
      " [-0.01308324 -0.02944615 -0.04190268 ... -0.09199509 -0.03737338\n",
      "  -0.0405663 ]\n",
      " [ 0.00802164 -0.02482525 -0.02637029 ... -0.01610369  0.00195497\n",
      "   0.01651701]]\n",
      "128 32\n",
      "[[ 0.04301016  0.02948082  0.00935837 ... -0.00646872  0.03375939\n",
      "  -0.04576859]\n",
      " [ 0.025393    0.01938764  0.0187553  ...  0.02809391  0.00445712\n",
      "  -0.00931548]\n",
      " [ 0.00193058 -0.03970129 -0.01796028 ...  0.04740249 -0.00832203\n",
      "  -0.00067864]\n",
      " ...\n",
      " [ 0.01283692  0.01815203 -0.04665133 ... -0.03296039  0.03845068\n",
      "   0.02637332]\n",
      " [ 0.02237841 -0.00752508  0.02284264 ... -0.01389819  0.02680524\n",
      "  -0.02358806]\n",
      " [ 0.02205449  0.03353945 -0.00692357 ...  0.06080664  0.05270093\n",
      "  -0.00709063]]\n"
     ]
    }
   ],
   "source": [
    "for i in candidates.take(2).as_numpy_iterator():\n",
    "    print(len(i), len(i[0]))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0b639d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self, layer_sizes):\n",
    "    super().__init__()\n",
    "    self.query_model = QueryModel(layer_sizes)\n",
    "    self.candidate_model = CandidateModel(layer_sizes)\n",
    "    self.task = tfrs.tasks.Retrieval(\n",
    "        metrics=tfrs.metrics.FactorizedTopK(\n",
    "            candidates=movies.batch(128).map(self.candidate_model),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    # We only pass the user id and timestamp features into the query model. This\n",
    "    # is to ensure that the training inputs would have the same keys as the\n",
    "    # query inputs. Otherwise the discrepancy in input structure would cause an\n",
    "    # error when loading the query model after saving it.\n",
    "    query_embeddings = self.query_model({\n",
    "        \"user_id\": features[\"user_id\"],\n",
    "        \"timestamp\": features[\"timestamp\"],\n",
    "    })\n",
    "    movie_embeddings = self.candidate_model({\n",
    "        \"movie_id\": features[\"movie_id\"],\n",
    "        \"movie_title\": features[\"movie_title\"],\n",
    "    })     # \"\"\" This is where the problem is \"\"\"\n",
    "\n",
    "    return self.task(query_embeddings, movie_embeddings, compute_metrics=not training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a3f23123",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(2048)\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6529080c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_title': array([b'Godfather, The (1972)', b'Escape to Witch Mountain (1975)',\n",
      "       b'Fargo (1996)', ..., b'Dangerous Minds (1995)',\n",
      "       b'Clueless (1995)', b'Scream (1996)'], dtype=object), 'user_id': array([b'424', b'429', b'53', ..., b'766', b'125', b'478'], dtype=object), 'timestamp': array([880859493, 882386848, 879442537, ..., 891310875, 892836551,\n",
      "       889388862]), 'movie_id': array([b'127', b'1133', b'100', ..., b'366', b'367', b'288'], dtype=object)}\n",
      "{'movie_title': array([b'Vertigo (1958)', b\"Singin' in the Rain (1952)\",\n",
      "       b'Saint, The (1997)', ..., b'Bound (1996)', b'Hoodlum (1997)',\n",
      "       b'Bio-Dome (1996)'], dtype=object), 'user_id': array([b'326', b'370', b'287', ..., b'663', b'489', b'595'], dtype=object), 'timestamp': array([879875432, 879434666, 875333873, ..., 889492503, 891447522,\n",
      "       886921977]), 'movie_id': array([b'479', b'705', b'748', ..., b'129', b'299', b'368'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "for i in cached_train.take(2).as_numpy_iterator():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c0e63e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 11s 194ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 14703.0897 - regularization_loss: 0.0000e+00 - total_loss: 14703.0897\n",
      "Epoch 2/3\n",
      "39/40 [============================>.] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 14678.5415 - regularization_loss: 0.0000e+00 - total_loss: 14678.5415WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 20s 466ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13990.0583 - regularization_loss: 0.0000e+00 - total_loss: 13990.0583 - val_factorized_top_k/top_1_categorical_accuracy: 0.0011 - val_factorized_top_k/top_5_categorical_accuracy: 0.0084 - val_factorized_top_k/top_10_categorical_accuracy: 0.0196 - val_factorized_top_k/top_50_categorical_accuracy: 0.1132 - val_factorized_top_k/top_100_categorical_accuracy: 0.2265 - val_loss: 28008.1719 - val_regularization_loss: 0.0000e+00 - val_total_loss: 28008.1719\n",
      "Epoch 3/3\n",
      "40/40 [==============================] - 9s 175ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 13720.4589 - regularization_loss: 0.0000e+00 - total_loss: 13720.4589\n",
      "Top-100 accuracy: 0.23.\n",
      "5/5 [==============================] - 8s 2s/step - factorized_top_k/top_1_categorical_accuracy: 9.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0087 - factorized_top_k/top_10_categorical_accuracy: 0.0207 - factorized_top_k/top_50_categorical_accuracy: 0.1214 - factorized_top_k/top_100_categorical_accuracy: 0.2350 - loss: 30771.6263 - regularization_loss: 0.0000e+00 - total_loss: 30771.6263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.0009500000160187483,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.008700000122189522,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.02070000022649765,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.12139999866485596,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.23499999940395355,\n",
       " 'loss': 27970.861328125,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 27970.861328125}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "model = MovielensModel([32])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "one_layer_history = model.fit(\n",
    "    cached_train,\n",
    "    validation_data=cached_test,\n",
    "    validation_freq=2,\n",
    "    epochs=num_epochs,\n",
    "    verbose=1)\n",
    "\n",
    "accuracy = one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
    "print(f\"Top-100 accuracy: {accuracy:.2f}.\")\n",
    "\n",
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935307a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
